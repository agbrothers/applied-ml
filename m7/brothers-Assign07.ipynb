{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing with the previous machine learning problem, let's get back to the pre-processed dataset Suicide Rates Overview 1985 to 2016 file. We would like to have a machine learning model to predict the suicide rate 'suicides/100k pop'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PREPROCESS DATASET\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "## SET RANDOM SEED\n",
    "np.random.seed(0)\n",
    "\n",
    "## LOAD DATASET\n",
    "df = pd.read_csv(\"master.csv\")\n",
    "\n",
    "## DROP POOR FEATURES AND GET ONE-HOT ENCODINGS\n",
    "clean_df = df.drop(columns=[' gdp_for_year ($) ', 'country-year','HDI for year', 'country'])\n",
    "clean_df = pd.get_dummies(clean_df, columns=['sex', 'age', 'generation'])\n",
    "\n",
    "## REMOVE VARIABLES FROM WHICH THE DEPENDENT VARIABLE IS DERIVED\n",
    "clean_df = clean_df.drop(columns=['suicides_no'])\n",
    "\n",
    "## REMOVE 2016 DATA\n",
    "clean_df = clean_df[clean_df[\"year\"] != 2016]\n",
    "\n",
    "## MOVE DEPENDENT VARIABLE TO LAST COLUMN\n",
    "cols = clean_df.columns.tolist()\n",
    "cols.remove(\"suicides/100k pop\")\n",
    "cols.append(\"suicides/100k pop\")\n",
    "clean_df = clean_df[cols]\n",
    "\n",
    "## MIN/MAX NORMALIZE DATA\n",
    "clean_df = clean_df.astype(np.float32)\n",
    "mn = clean_df.min().values\n",
    "mx = clean_df.max().values\n",
    "norm = lambda x, mn, mx: (x - mn) / (mx-mn+1e-10)\n",
    "unnorm = lambda x, mn, mx: (x * (mx-mn+1e-10)) + mn\n",
    "norm_df = norm(clean_df, mn, mx)\n",
    "\n",
    "# Prepare the input X matrix and target y vector\n",
    "X = norm_df.loc[:, norm_df.columns != 'suicides/100k pop'].values\n",
    "y = norm_df.loc[:, norm_df.columns == 'suicides/100k pop'].values.ravel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: I removed `Country` to significantly reduce the size of the feature space (+memory footprint), and since it doesn't have a very meaningful numeric integer encoding scheme. I am also using the given `suicides/100k pop` feature as the target for the regression problem, as opposed to the binary category I derived by quantizing and binning the same feature on the classification assignment. The rest of the pre-processing is identical to what I did in Module 3. Please see my submission for assignment 3 for additional plots and reasoning behind the feature selection and cleaning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. [10 pts] Use your previous pre-processed dataset, keep the variables as one-hot encoded, and develop a multiple linear regression model. How many regression coefficients does this model have? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.0459\n",
      "MSE: 0.0052\n",
      "Number of coefficients: 18\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, train_size=0.8)\n",
    "model_config = {\"fit_intercept\":True}\n",
    "model = LinearRegression(**model_config)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"MAE: {metrics.mean_absolute_error(y_test, y_pred):.4f}\")\n",
    "print(f\"MSE: {metrics.mean_squared_error(y_test, y_pred):.4f}\")\n",
    "print(f\"Number of coefficients: {model.n_features_in_+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. [10 pts] Use this model to predict the target variable for people with age 20, male, and generation X. Report this prediction. What is the MAE error of this prediction?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted suicides/100k pop: 20.5034\n",
      "MAE: 0.0158\n"
     ]
    }
   ],
   "source": [
    "## LOOKUP GROUND TRUTH SAMPLES WITH THE SPECIFIED FEATURES\n",
    "truth = clean_df[clean_df[\"sex_male\"] == 1.]\n",
    "truth = truth[truth[\"age_15-24 years\"] == 1.]\n",
    "truth = truth[truth[\"generation_Generation X\"] == 1.]\n",
    "truth = np.stack(truth.values)[..., :-1]\n",
    "norm_truth = norm(truth, mn[:-1], mx[:-1])\n",
    "\n",
    "## PRODUCE RANDOM SAMPLES WITH THE SPECIFIED FEATURES\n",
    "y_pred = model.predict(norm_truth)\n",
    "y_true = np.mean(y_pred)\n",
    "\n",
    "## GENERATE RANDOM SAMPLES WITH THE SPECIFIED FEATURES\n",
    "sample = {key:0. for key in norm_df.keys() if key != \"suicides/100k pop\"}\n",
    "sample[\"age_15-24 years\"] = 1.\n",
    "sample[\"sex_male\"] = 1.\n",
    "sample[\"generation_Generation X\"] = 1.\n",
    "\n",
    "samples = []\n",
    "for _ in range(100):\n",
    "    sample.update({\n",
    "        \"year\": np.random.randint(mn[0],mx[0]),\n",
    "        \"population\": np.random.uniform(mn[1],mx[1]),\n",
    "        \"gdp_per_capita ($)\": np.random.uniform(mn[2],mx[2]),\n",
    "    })\n",
    "    x = np.array(list(sample.values()))\n",
    "    x_norm = norm(x, mn[:-1], mx[:-1])\n",
    "    samples.append(x_norm)\n",
    "samples = np.stack(samples)\n",
    "\n",
    "## PRODUCE RANDOM SAMPLES WITH THE SPECIFIED FEATURES\n",
    "y_pred = model.predict(samples)\n",
    "y_unnorm = unnorm(np.mean(y_pred), mn[-1], mx[-1])\n",
    "print(f\"Predicted suicides/100k pop: {y_unnorm:.4f}\")\n",
    "mae = np.mean(np.abs(y_true - y_pred))\n",
    "print(f\"MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. [20 pts] Now go back to the original sex, age, and generation variables in their original numerical form (i.e. prior to the one-hot encoding) and build a new model. I.e., feature engineer the original nominal age and generation features into truly numerical features.) How many line coefficients are there? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.0463\n",
      "MSE: 0.0053\n",
      "Number of coefficients: 7\n"
     ]
    }
   ],
   "source": [
    "## DROP POOR FEATURES\n",
    "clean_df = df.drop(columns=[' gdp_for_year ($) ', 'country-year','HDI for year', 'country'])\n",
    "\n",
    "## CONVERT CATEGORICAL VARIABLES TO NUMERIC\n",
    "categorical = [\"sex\",\"age\",\"generation\"]\n",
    "mapping = {}\n",
    "mapping[\"sex\"] = {\n",
    "    \"male\": 0.,\n",
    "    \"female\": 1.,\n",
    "}\n",
    "mapping[\"age\"] = {\n",
    "    \"75+ years\": 0.,\n",
    "    \"55-74 years\": 1.,\n",
    "    \"35-54 years\": 2.,\n",
    "    \"25-34 years\": 3.,\n",
    "    \"15-24 years\": 4.,\n",
    "    \"5-14 years\": 5.,\n",
    "}\n",
    "mapping[\"generation\"] = {\n",
    "    \"G.I. Generation\": 0.,   # GI Generation – 1901-1927.\n",
    "    \"Silent\": 1.,            # Silent Generation – 1928-1945.\n",
    "    \"Boomers\": 2.,           # Baby Boomers – 1946-1964.\n",
    "    \"Generation X\": 3.,      # Generation X – 1965 - 1980.\n",
    "    \"Millenials\": 4.,        # Millennials – 1981-1996.\n",
    "    \"Generation Z\": 5.,      # Generation Z – 1997-2012.\n",
    "    \"Generation Alpha\": 6.,  # Generation Alpha – 2013 - present.\n",
    "}\n",
    "\n",
    "for feat in categorical: \n",
    "    clean_df[feat] = clean_df[feat].apply(lambda x: mapping[feat][x])\n",
    "\n",
    "## REMOVE VARIABLES FROM WHICH THE DEPENDENT VARIABLE IS DERIVED\n",
    "clean_df = clean_df.drop(columns=['suicides_no'])\n",
    "\n",
    "## REMOVE 2016 DATA\n",
    "clean_df = clean_df[clean_df[\"year\"] != 2016]\n",
    "\n",
    "## MOVE DEPENDENT VARIABLE TO LAST COLUMN\n",
    "cols = clean_df.columns.tolist()\n",
    "cols.remove(\"suicides/100k pop\")\n",
    "cols.append(\"suicides/100k pop\")\n",
    "clean_df = clean_df[cols]\n",
    "\n",
    "## MIN/MAX NORMALIZE DATA\n",
    "clean_df = clean_df.astype(np.float32)\n",
    "mn = clean_df.min().values\n",
    "mx = clean_df.max().values\n",
    "norm = lambda x, mn, mx: (x - mn) / (mx-mn+1e-10)\n",
    "unnorm = lambda x, mn, mx: (x * (mx-mn+1e-10)) + mn\n",
    "norm_df = norm(clean_df, mn, mx)\n",
    "\n",
    "# Prepare the input X matrix and target y vector\n",
    "X = norm_df.loc[:, norm_df.columns != 'suicides/100k pop'].values\n",
    "y = norm_df.loc[:, norm_df.columns == 'suicides/100k pop'].values.ravel()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, train_size=0.8)\n",
    "model_config = {\"fit_intercept\":True}\n",
    "model = LinearRegression(**model_config)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"MAE: {metrics.mean_absolute_error(y_test, y_pred):.4f}\")\n",
    "print(f\"MSE: {metrics.mean_squared_error(y_test, y_pred):.4f}\")\n",
    "print(f\"Number of coefficients: {model.n_features_in_+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. [10 pts] Use this new Q3. model to predict the target value for the people with age 20, male, and generation X. Report the prediction. What is the MAE error of this prediction?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted suicides/100k pop: 12.7163\n",
      "MAE: 0.0101\n"
     ]
    }
   ],
   "source": [
    "## LOOKUP GROUND TRUTH SAMPLES WITH THE SPECIFIED FEATURES\n",
    "truth = clean_df[clean_df[\"sex\"] == mapping[\"sex\"][\"male\"]]\n",
    "truth = truth[truth[\"age\"] == mapping[\"age\"][\"15-24 years\"]]\n",
    "truth = truth[truth[\"generation\"] == mapping[\"generation\"][\"Generation X\"]]\n",
    "truth = np.stack(truth.values)[..., :-1]\n",
    "norm_truth = norm(truth, mn[:-1], mx[:-1])\n",
    "\n",
    "## PRODUCE RANDOM SAMPLES WITH THE SPECIFIED FEATURES\n",
    "y_pred = model.predict(norm_truth)\n",
    "y_true = np.mean(y_pred)\n",
    "\n",
    "## GENERATE RANDOM SAMPLES WITH THE SPECIFIED FEATURES\n",
    "sample = {key:0. for key in norm_df.keys() if key != \"suicides/100k pop\"}\n",
    "sample[\"sex\"] = mapping[\"sex\"][\"male\"]\n",
    "sample[\"age\"] = mapping[\"age\"][\"15-24 years\"]\n",
    "sample[\"generation\"] = mapping[\"generation\"][\"Generation X\"]\n",
    "\n",
    "samples = []\n",
    "for _ in range(100):\n",
    "    sample.update({\n",
    "        \"year\": np.random.randint(mn[0],mx[0]),\n",
    "        \"population\": np.random.uniform(mn[1],mx[1]),\n",
    "        \"gdp_per_capita ($)\": np.random.uniform(mn[2],mx[2]),\n",
    "    })\n",
    "    x = np.array(list(sample.values()))\n",
    "    x_norm = norm(x, mn[:-1], mx[:-1])\n",
    "    samples.append(x_norm)\n",
    "samples = np.stack(samples)\n",
    "\n",
    "## PRODUCE RANDOM SAMPLES WITH THE SPECIFIED FEATURES\n",
    "y_pred = model.predict(samples)\n",
    "y_unnorm = unnorm(np.mean(y_pred), mn[-1], mx[-1])\n",
    "print(f\"Predicted suicides/100k pop: {y_unnorm:.4f}\")\n",
    "mae = np.mean(np.abs(y_true - y_pred))\n",
    "print(f\"MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. [10 pts] Did you note any change in these two model performances?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the one-hot encoded categorical variables we had a MAE of 0.0158 and with the numeric integer encoded categorical variables we had an MAE of 0.0101. That's an approximate 36% reduction in the Mean Absolute Error, which is a very good gain in terms of generalization performance. \n",
    "\n",
    "NOTE: My initial approach for this problem was to just use the built-in pandas functions to map the categorical variables to numeric integers. However when I examined the mapping, the categories were ordered by name rather than by their proper logical ordering (i.e. ascending/descending for age and generation). However, the performance of the model using this arbitrary ordering was significantly better, around 0.0032 on average. This is the opposite of what I would have expected, and I'm not sure why logically ordering the numerical mappings made the model worse. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. [10 pts] Use your Q3. model to predict the target value for age 33, male, and generation Alpha (i.e. the generation after generation Z); report the prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted suicides/100k pop: 25.3154\n"
     ]
    }
   ],
   "source": [
    "## GENERATE RANDOM SAMPLES WITH THE SPECIFIED FEATURES\n",
    "sample = {key:0. for key in norm_df.keys() if key != \"suicides/100k pop\"}\n",
    "sample[\"sex\"] = mapping[\"sex\"][\"male\"]\n",
    "sample[\"age\"] = mapping[\"age\"][\"25-34 years\"]\n",
    "sample[\"generation\"] = mapping[\"generation\"][\"Generation Alpha\"]\n",
    "\n",
    "samples = []\n",
    "for _ in range(100):\n",
    "    sample.update({\n",
    "        \"year\": np.random.randint(mn[0],mx[0]),\n",
    "        \"population\": np.random.uniform(mn[1],mx[1]),\n",
    "        \"gdp_per_capita ($)\": np.random.uniform(mn[2],mx[2]),\n",
    "    })\n",
    "    x = np.array(list(sample.values()))\n",
    "    x_norm = norm(x, mn[:-1], mx[:-1])\n",
    "    samples.append(x_norm)\n",
    "samples = np.stack(samples)\n",
    "\n",
    "## PRODUCE RANDOM SAMPLES WITH THE SPECIFIED FEATURES\n",
    "y_pred = model.predict(samples)\n",
    "y_unnorm = unnorm(np.mean(y_pred), mn[-1], mx[-1])\n",
    "print(f\"Predicted suicides/100k pop: {y_unnorm:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. [10 pts] Give one advantage when using regression (as opposed to classification with nominal features) in terms of independent variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because regression does not enforce the assumption of independence between features (unlike classification methods like Naïve Bayes), we can take full advantage of features which may have some relationship or correlation. This allows us to utilize a greater diversity of features, resulting in additional predictive signal in the dataset for the model to learn from. This means we are more likely to end up with better model given the same data source. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. [10 pts] Give one advantage when using regular numerical values rather than one-hot encoding for regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I believe the most beneficial advantage is the ability to generalize to never-before-seen categories as shown in problem 6. The ability to include Generation Alpha, not break the model, and get a meaningful prediction is a huge boost to the generalization capability of the model. The only caveat is that this benefit is restricted to ordinal variables. Other categorical variables that have no inherient order, such as `Country`, cannot take advantage of this. If there is no relation between integer enodings for `Country`, adding a new country with the encoding n+1 does not tell the model anything meaningful for making a prediction given that never-before-seen country. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. [10 pts] Now that you developed both a classifier (previously) and a regression model for the problem in this assignment, which method do you suggest to your machine learning model customer? Classifier or regression? Why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the dependent variable for this dataset is continuous, and because there are a number of useful ordinal nominal variables, I would choose regression in general. Linear regression also provides other benefits which may be very helpful when conducting analysis of suicide rates, such as the ability to investigate feature influence. By determining if any feature is strongly positively or negatively correlated with suicide rate, we can derive methods for reducing suicides in a given population. \n",
    "\n",
    "If there was a task that was specifically looking to classify if a group was at risk or a suicide rate was over a certain threshold, I believe Logistic Regression would be an excellent approach. However, I still think standard regression is a much better fit for this dataset than classification. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
