{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment 11 \n",
    "Applied Machine Learning \n",
    "\n",
    "1. [50 pts] In this assignment, we will use a priori analysis to find phrases, or interesting word \n",
    "patterns, in a novel. \n",
    "\n",
    "Note that you are free to use any a priori analytics and algorithm library in this assignment. \n",
    " \n",
    "Use the nltk library corpus gutenberg API and load the novel 'carroll-alice.txt', which is Alice in Wonderland by Lewis Carroll (although his real name was Charles Dodgson). There are 1703 sentences in the novel—which can be represented as 1703 transactions. Use any means you like to parse/extract words and save in a .csv format to be read by Weka framework, similar to the a priori Analysis module. (Hint: Feel free to use mlxtend library instead of Weka.) \n",
    " \n",
    "Hint: Removing stop words and symbols using regular expressions can be helpful: \n",
    "from nltk.corpus import gutenberg, stopwords \n",
    "Stop_words = stopwords.words('english') \n",
    "Sentences = gutenberg.sents('carroll-alice.txt') \n",
    "TermsSentences = [] \n",
    "for terms in Sentences: \n",
    "  terms = [w for w in terms if w not in Stop_words] \n",
    "  terms = [w for w in terms if re.search(r'^[a-zA-Z]{2}', w) is not None] \n",
    " \n",
    "If you chose to Weka, use FPGrowth and start with default parameters. Reduce \n",
    "lowerBoundMinSupport to reach to a sweet point for the support and avoid exploding the \n",
    "number of rules generated. \n",
    " \n",
    "Report interesting patterns. \n",
    " \n",
    "(Example: Some of the frequently occurring phrases are “Mock Turtle”, “White Rabbit”, etc.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1091)>\n",
      "[nltk_data] Error loading gutenberg: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1091)>\n",
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1091)>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "import pandas as pd\n",
    "\n",
    "## DOWNLOAD NLTK DATA\n",
    "nltk.download('stopwords')\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')\n",
    "\n",
    "## LOAD STOPWORDS + SENTENCES\n",
    "stopwords = set(stopwords.words('english'))\n",
    "sentences = gutenberg.sents('carroll-alice.txt')\n",
    "\n",
    "## REMOVE STOPWORDS AND SYMBOLS VIA REGEX\n",
    "processed_sentences = []\n",
    "for terms in sentences:\n",
    "    terms = [w.lower() for w in terms if w.lower() not in stopwords]\n",
    "    terms = [re.sub(r'[^a-zA-Z]', '', w) for w in terms if re.search(r'^[a-zA-Z]{2}', w)]\n",
    "    processed_sentences.append(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>support</th>\n",
       "      <th>itemsets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.267176</td>\n",
       "      <td>(said)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.228420</td>\n",
       "      <td>(alice)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.095713</td>\n",
       "      <td>(alice, said)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.068115</td>\n",
       "      <td>(little)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.057546</td>\n",
       "      <td>(one)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.048150</td>\n",
       "      <td>(like)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.048150</td>\n",
       "      <td>(know)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.048150</td>\n",
       "      <td>(went)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.043453</td>\n",
       "      <td>(thought)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.042866</td>\n",
       "      <td>(queen)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.041691</td>\n",
       "      <td>(could)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.041104</td>\n",
       "      <td>(would)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.039342</td>\n",
       "      <td>(time)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.038168</td>\n",
       "      <td>(see)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0.036406</td>\n",
       "      <td>(well)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.035232</td>\n",
       "      <td>(king)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.034058</td>\n",
       "      <td>(alice, thought)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.034058</td>\n",
       "      <td>(turtle)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.033470</td>\n",
       "      <td>(began)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.032883</td>\n",
       "      <td>(turtle, mock)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.032883</td>\n",
       "      <td>(mock)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.032883</td>\n",
       "      <td>(hatter)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.031709</td>\n",
       "      <td>(gryphon)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.031122</td>\n",
       "      <td>(think)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.029947</td>\n",
       "      <td>(say)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.029360</td>\n",
       "      <td>(first)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.029360</td>\n",
       "      <td>(way)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.029360</td>\n",
       "      <td>(go)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.029360</td>\n",
       "      <td>(much)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.029360</td>\n",
       "      <td>(quite)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      support          itemsets\n",
       "82   0.267176            (said)\n",
       "1    0.228420           (alice)\n",
       "128  0.095713     (alice, said)\n",
       "51   0.068115          (little)\n",
       "71   0.057546             (one)\n",
       "50   0.048150            (like)\n",
       "46   0.048150            (know)\n",
       "110  0.048150            (went)\n",
       "97   0.043453         (thought)\n",
       "75   0.042866           (queen)\n",
       "11   0.041691           (could)\n",
       "116  0.041104           (would)\n",
       "100  0.039342            (time)\n",
       "84   0.038168             (see)\n",
       "109  0.036406            (well)\n",
       "45   0.035232            (king)\n",
       "132  0.034058  (alice, thought)\n",
       "103  0.034058          (turtle)\n",
       "6    0.033470           (began)\n",
       "149  0.032883    (turtle, mock)\n",
       "61   0.032883            (mock)\n",
       "40   0.032883          (hatter)\n",
       "36   0.031709         (gryphon)\n",
       "96   0.031122           (think)\n",
       "83   0.029947             (say)\n",
       "27   0.029360           (first)\n",
       "108  0.029360             (way)\n",
       "31   0.029360              (go)\n",
       "64   0.029360            (much)\n",
       "76   0.029360           (quite)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## ENCODE SENTENCES LIKE TRANSACTIONS\n",
    "encoder = TransactionEncoder()\n",
    "onehot = encoder.fit(processed_sentences).transform(processed_sentences)\n",
    "df = pd.DataFrame(onehot, columns=encoder.columns_)\n",
    "\n",
    "## APPLY APRIORI ALGORITHM TO GET FREQUENT ITEMS\n",
    "freq_itemsets = apriori(df, min_support=0.01, use_colnames=True)\n",
    "freq_itemsets.sort_values(by=\"support\", ascending=False).head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in lecture, an itemset's support is bounded by the minimum support of any of the subsets that compose it. Thus it should be no surprise that the itemsets with the highest support are atomic, containing only one word. \n",
    "\n",
    "Looking at the itemsets with highest support displays the words the author uses most frequently. The most common word is \"said\", which is followed by \"alice\", the main character, and (\"alice\", \"said\") which captures all instances of alice talking. This alone can tell us that the novel is written heavily around dialogue between characters, and judging by the support values of those top 3 words, much of that dialogue is dominated by alice. \n",
    "\n",
    "This is interesting on its own, but doesn't tell us anything about words that are seen frequently together. There are some exceptions, like (alice, said) in the top 30, which makes sense. Phrases that combine two words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>support</th>\n",
       "      <th>itemsets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16945</th>\n",
       "      <td>0.002936</td>\n",
       "      <td>(turtle, mock, said, course)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17006</th>\n",
       "      <td>0.002936</td>\n",
       "      <td>(little, golden, key, door)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15851</th>\n",
       "      <td>0.002349</td>\n",
       "      <td>(alice, said, would, know)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17218</th>\n",
       "      <td>0.002349</td>\n",
       "      <td>(kid, gloves, fan, white)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17347</th>\n",
       "      <td>0.002349</td>\n",
       "      <td>(little, glass, table, found)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16003</th>\n",
       "      <td>0.002349</td>\n",
       "      <td>(turtle, mock, alice, said)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15889</th>\n",
       "      <td>0.002349</td>\n",
       "      <td>(alice, little, said, like)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15949</th>\n",
       "      <td>0.002349</td>\n",
       "      <td>(tell, little, alice, said)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16095</th>\n",
       "      <td>0.002349</td>\n",
       "      <td>(tell, alice, would, said)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16541</th>\n",
       "      <td>0.002349</td>\n",
       "      <td>(ootiful, beau, oop, soo)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17719</th>\n",
       "      <td>0.002349</td>\n",
       "      <td>(turtle, mock, said, gryphon)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15824</th>\n",
       "      <td>0.001762</td>\n",
       "      <td>(alice, jumped, moment, last)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17434</th>\n",
       "      <td>0.001762</td>\n",
       "      <td>(little, glass, key, table)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17422</th>\n",
       "      <td>0.001762</td>\n",
       "      <td>(glass, golden, key, table)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17988</th>\n",
       "      <td>0.001762</td>\n",
       "      <td>(majesty, said, white, rabbit)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17215</th>\n",
       "      <td>0.001762</td>\n",
       "      <td>(kid, gloves, fan, pair)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15922</th>\n",
       "      <td>0.001762</td>\n",
       "      <td>(alice, would, like, thought)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16232</th>\n",
       "      <td>0.001762</td>\n",
       "      <td>(got, away, could, soon)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15339</th>\n",
       "      <td>0.001762</td>\n",
       "      <td>(began, rabbit, alice, white)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16032</th>\n",
       "      <td>0.001762</td>\n",
       "      <td>(thought, alice, never, seen)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16226</th>\n",
       "      <td>0.001762</td>\n",
       "      <td>(could, away, found, soon)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15180</th>\n",
       "      <td>0.001762</td>\n",
       "      <td>(alice, like, round, another)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16224</th>\n",
       "      <td>0.001762</td>\n",
       "      <td>(could, away, got, found)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17002</th>\n",
       "      <td>0.001762</td>\n",
       "      <td>(little, glass, table, door)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18957</th>\n",
       "      <td>0.001762</td>\n",
       "      <td>(evening, oop, beautiful, soup, soo)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15911</th>\n",
       "      <td>0.001762</td>\n",
       "      <td>(alice, see, said, like)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19282</th>\n",
       "      <td>0.001762</td>\n",
       "      <td>(kid, gloves, white, pair, fan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15460</th>\n",
       "      <td>0.001762</td>\n",
       "      <td>(alice, seemed, see, could)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17431</th>\n",
       "      <td>0.001762</td>\n",
       "      <td>(little, glass, table, hall)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15576</th>\n",
       "      <td>0.001762</td>\n",
       "      <td>(little, alice, said, felt)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        support                              itemsets\n",
       "16945  0.002936          (turtle, mock, said, course)\n",
       "17006  0.002936           (little, golden, key, door)\n",
       "15851  0.002349            (alice, said, would, know)\n",
       "17218  0.002349             (kid, gloves, fan, white)\n",
       "17347  0.002349         (little, glass, table, found)\n",
       "16003  0.002349           (turtle, mock, alice, said)\n",
       "15889  0.002349           (alice, little, said, like)\n",
       "15949  0.002349           (tell, little, alice, said)\n",
       "16095  0.002349            (tell, alice, would, said)\n",
       "16541  0.002349             (ootiful, beau, oop, soo)\n",
       "17719  0.002349         (turtle, mock, said, gryphon)\n",
       "15824  0.001762         (alice, jumped, moment, last)\n",
       "17434  0.001762           (little, glass, key, table)\n",
       "17422  0.001762           (glass, golden, key, table)\n",
       "17988  0.001762        (majesty, said, white, rabbit)\n",
       "17215  0.001762              (kid, gloves, fan, pair)\n",
       "15922  0.001762         (alice, would, like, thought)\n",
       "16232  0.001762              (got, away, could, soon)\n",
       "15339  0.001762         (began, rabbit, alice, white)\n",
       "16032  0.001762         (thought, alice, never, seen)\n",
       "16226  0.001762            (could, away, found, soon)\n",
       "15180  0.001762         (alice, like, round, another)\n",
       "16224  0.001762             (could, away, got, found)\n",
       "17002  0.001762          (little, glass, table, door)\n",
       "18957  0.001762  (evening, oop, beautiful, soup, soo)\n",
       "15911  0.001762              (alice, see, said, like)\n",
       "19282  0.001762       (kid, gloves, white, pair, fan)\n",
       "15460  0.001762           (alice, seemed, see, could)\n",
       "17431  0.001762          (little, glass, table, hall)\n",
       "15576  0.001762           (little, alice, said, felt)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## LOWER THRESHOLD AND SHOW LARGER ITEMSETS\n",
    "large_itemsets = apriori(df, min_support=0.001, use_colnames=True)\n",
    "large_itemsets = large_itemsets[large_itemsets[\"itemsets\"].apply(len) > 3]\n",
    "large_itemsets.sort_values(by=\"support\", ascending=False).head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Larger itemsets are much more interesting, as we can now use them to glean many more details about the characters, recurring things, and how the author's favorite words are combined. For example, (alice, would, said, know) further emphasizes that alice is speaking or otherwise narrating heavily. It is also interesting that the vast majority still contain \"said\", emphasizing that the novel is dialogue heavy. We also see (mock, said, gryphon, turtle), likely indicating frequent dialogue between gryphon and turtle characters. \n",
    "\n",
    "While relatively simple, this type of analysis already yields very interesting results on a toy problem and would be very handy when exploring new data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. [50 pts] In the lecture module, the class NeuralNetMLP implements a neural network with a single hidden layer. Make the necessary modifications to upgrade it to a 2-hidden layer neural network. Run it on the MNIST dataset and report its performance. \n",
    " \n",
    "(Hint: Raschka, Chapter 12) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows= 60000, columns= 784\n",
      "Rows= 10000, columns= 784\n"
     ]
    }
   ],
   "source": [
    "## DATASET LOADING FUNCTION FROM THE LECTURE NOTES\n",
    "def load_mnist(path, kind='train'):\n",
    "    from numpy import fromfile, uint8\n",
    "    import os\n",
    "    import struct\n",
    "    \n",
    "    labels_path = os.path.join(path, '%s-labels-idx1-ubyte' % kind)\n",
    "    images_path = os.path.join(path, '%s-images-idx3-ubyte' % kind)\n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II', lbpath.read(8))\n",
    "        labels = fromfile(lbpath, dtype=uint8)\n",
    "        with open(images_path, 'rb') as imgpath:\n",
    "            magic, num, rows, cols = struct.unpack(\">IIII\",imgpath.read(16))\n",
    "            images = fromfile(imgpath, dtype=uint8).reshape(len(labels), 784)\n",
    "            images = ((images / 255.) - .5) * 2\n",
    "    return images, labels\n",
    "\n",
    "X_train_mnist, y_train_mnist = load_mnist('mnist', kind='train')\n",
    "print(f'Rows= {X_train_mnist.shape[0]}, columns= {X_train_mnist.shape[1]}')\n",
    "\n",
    "X_test_mnist, y_test_mnist = load_mnist('mnist', kind='t10k')\n",
    "print(f'Rows= {X_test_mnist.shape[0]}, columns= {X_test_mnist.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MLP CLASS FROM LECTURE \n",
    "## MODIFIED WITH A SECOND HIDDEN LAYER\n",
    "import numpy as np\n",
    "\n",
    "class NeuralNetMLP(object):\n",
    "\n",
    "    def __init__(self, n_hidden=30, epochs=100, eta=0.001, minibatch_size=1, seed=None):\n",
    "        self.random = np.random.RandomState(seed)  # used to randomize weights\n",
    "        self.n_hidden = n_hidden  # size of the hidden layer\n",
    "        self.epochs = epochs  # number of iterations\n",
    "        self.eta = eta  # learning rate\n",
    "        self.minibatch_size = minibatch_size  # size of training batch - 1 would not work\n",
    "        self.w_out, self.w_h1, self.w_h2 = None, None, None\n",
    "    \n",
    "    @staticmethod\n",
    "    def onehot(_y, _n_classes):  # one hot encode the input class y\n",
    "        onehot = np.zeros((_n_classes, _y.shape[0]))\n",
    "        for idx, val in enumerate(_y.astype(int)):\n",
    "            onehot[val, idx] = 1.0\n",
    "        return onehot.T\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(_z):  # Eq 1\n",
    "        return 1.0 / (1.0 + np.exp(-np.clip(_z, -250, 250)))\n",
    "\n",
    "    def _forward(self, _X):  # Eq 2\n",
    "        z_h1 = np.dot(_X, self.w_h1)\n",
    "        a_h1 = self.sigmoid(z_h1)\n",
    "        z_h2 = np.dot(a_h1, self.w_h2)\n",
    "        a_h2 = self.sigmoid(z_h2)\n",
    "        z_out = np.dot(a_h2, self.w_out)\n",
    "        a_out = self.sigmoid(z_out)\n",
    "        return a_h1, a_h2, a_out, z_out\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_cost(y_enc, output):  # Eq 4\n",
    "        term1 = -y_enc * (np.log(output))\n",
    "        term2 = (1.0-y_enc) * np.log(1.0-output)\n",
    "        cost = np.sum(term1 - term2)\n",
    "        return cost\n",
    "\n",
    "    def predict(self, _X):\n",
    "        a_h1, a_h2, a_out, z_out = self._forward(_X)\n",
    "        ypred = np.argmax(z_out, axis=1)\n",
    "        return ypred\n",
    "\n",
    "    def fit(self, _X_train, _y_train, _X_valid, _y_valid):\n",
    "        import sys\n",
    "        n_output = np.unique(_y_train).shape[0]  # number of class labels\n",
    "        n_features = _X_train.shape[1]\n",
    "        self.w_out = self.random.normal(loc=0.0, scale=0.1, size=(self.n_hidden, n_output))\n",
    "        self.w_h2 = self.random.normal(loc=0.0, scale=0.1, size=(self.n_hidden, self.n_hidden))\n",
    "        self.w_h1 = self.random.normal(loc=0.0, scale=0.1, size=(n_features, self.n_hidden))\n",
    "        y_train_enc = self.onehot(_y_train, n_output)  # one-hot encode original y\n",
    "        for ei in range(self.epochs):  # Ideally must shuffle at every epoch\n",
    "            indices = np.arange(_X_train.shape[0])\n",
    "            for start_idx in range(0, indices.shape[0] - self.minibatch_size + 1, self.minibatch_size):\n",
    "                batch_idx = indices[start_idx:start_idx + self.minibatch_size]\n",
    "                \n",
    "                a_h1, a_h2, a_out, z_out = self._forward(_X_train[batch_idx])  # neural network model\n",
    "                \n",
    "                sigmoid_derivative_h2 = a_h2 * (1.0-a_h2)  # Eq 3\n",
    "                sigmoid_derivative_h1 = a_h1 * (1.0-a_h1)  # Eq 3\n",
    "                \n",
    "                delta_out = a_out - y_train_enc[batch_idx]  # Eq 5\n",
    "                delta_h2 = (np.dot(delta_out, self.w_out.T) * sigmoid_derivative_h2)  # Eq 6\n",
    "                delta_h1 = (np.dot(delta_h2, self.w_h2.T) * sigmoid_derivative_h1)  # Eq 6\n",
    "                \n",
    "                grad_w_out = np.dot(a_h2.T, delta_out)  # Eq 7\n",
    "                grad_w_h2 = np.dot(a_h1.T, delta_h2)  # Eq 8\n",
    "                grad_w_h1 = np.dot(_X_train[batch_idx].T, delta_h1)  # Eq 8\n",
    "                \n",
    "                self.w_out -= self.eta*grad_w_out  # Eq 9\n",
    "                self.w_h2 -= self.eta*grad_w_h2  # Eq 9\n",
    "                self.w_h1 -= self.eta*grad_w_h1  # Eq 9\n",
    "\n",
    "            # Evaluation after each epoch during training\n",
    "            a_h1, a_h2, a_out, z_out = self._forward(_X_train)\n",
    "            cost = self.compute_cost(y_enc=y_train_enc, output=a_out)\n",
    "            y_train_pred = self.predict(_X_train)  # monitoring training progress through reclassification\n",
    "            y_valid_pred = self.predict(_X_valid)  # monitoring training progress through validation\n",
    "            train_acc = ((np.sum(_y_train == y_train_pred)).astype(float) / _X_train.shape[0])\n",
    "            valid_acc = ((np.sum(_y_valid == y_valid_pred)).astype(float) / _X_valid.shape[0])\n",
    "            sys.stderr.write('\\r%d/%d | Cost: %.2f ' '| Train/Valid Acc.: %.2f%%/%.2f%% '%\n",
    "                (ei+1, self.epochs, cost, train_acc*100, valid_acc*100))\n",
    "            sys.stderr.flush()\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300/300 | Cost: 7190.22 | Train/Valid Acc.: 98.25%/95.96%  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy= 95.04%\n",
      "[[ 950    0    0    1    4    6   14    2    3    0]\n",
      " [   0 1105    5    1    0    2    5    4   13    0]\n",
      " [   5    4  978   14    4    3    4    7   13    0]\n",
      " [   2    1   17  960    1    9    2    7    8    3]\n",
      " [   1    0    3    0  944    0    9    2    7   16]\n",
      " [  10    1    3   32    1  802   10    3   23    7]\n",
      " [   7    2    3    1    9   10  921    0    5    0]\n",
      " [   1    3   19   10    5    2    0  978    1    9]\n",
      " [   6    1    3   11    4    9    5    3  925    7]\n",
      " [   6    4    0   10   27    2    0    8   11  941]]\n"
     ]
    }
   ],
   "source": [
    "## TRAIN THE NN\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def get_acc(_y_test, _y_pred):\n",
    "    return (np.sum(_y_test == _y_pred)).astype(float) / _y_test.shape[0]\n",
    "\n",
    "nn = NeuralNetMLP(n_hidden=20, epochs=300, eta=0.0005, minibatch_size=100, seed=1)\n",
    "nn.fit(X_train_mnist[:55000], y_train_mnist[:55000], X_train_mnist[55000:], y_train_mnist[55000:]) ;\n",
    "\n",
    "y_pred = nn.predict(X_test_mnist)\n",
    "\n",
    "print(f'Accuracy= {get_acc(y_test_mnist, y_pred)*100:.2f}%')\n",
    "print(confusion_matrix(y_test_mnist, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
