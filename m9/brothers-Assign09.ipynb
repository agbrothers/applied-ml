{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment 9 \n",
    "Applied Machine Learning \n",
    "Please refer to the Reinforcement Learning Jupyter notebook in course materials.  \n",
    " \n",
    "Answer questions 1-3 below considering any Nim game reinforcement learning model. \n",
    " \n",
    "1. [10 pts] Describe the environment in the Nim learning model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nim is a competitive environment reinforcement learning environment involving two agents. The environment is initialized with a discrete number of piles, and each pile containing a discrete number of items. The players take turns, and on each turn they can choose a single pile and remove at least one item or more from it. Then the next player draws items from a pile, and so on. The player to clear all items from the final pile wins. Thus the goal is draw items from piles strategically so that your opponent can never be left with a single pile on their turn, otherwise they can draw all of the remaining items from the final pile and win. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. [10 pts] Describe the agent(s) in the Nim learning model (Hint, not just the Q-learner). Is Guru an agent?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, the agents are the players of the game. An agent here can be seen as any model which take as input a state and returns a valid action to the environment. The policy that the agent follows does not necessarily have to use the input state to compute its action, as is the case for a random agent. In the Nim environment, optimal actions are a function of the state, and the policy of the Guru agent exploits hard-coded expert knowledge to derive the optimal action from a given state. Finally, the Q-learner is another type of agent that learns an action policy through exploring and exploiting its environment. These three examples are all Nim agents with different policies, and there are a much larger number of policies possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. [10 pts] Describe the reward and penalty in the Nim learning model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As implemented in the Module 9 Jupyter Notebook, there are no rewards provided by the Nim \"game\" function by default. Usually, reinforcement learning environments provide rewards with each step, regardless of what policy is being used. Here, a positive reward of +100 is hard-coded externally and used by the nim_qlearn function each time the q-learner agent reaches the win state on its own move. There appears to be no negative penalty, so positive incentive is the only motivating factor in this environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. [10 pts] How many possible states there could be in the Nim game with a maximum of 10 items per pile and 3 piles total? (This problem requires a number for its answer, not merely a closed-form expression.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There would be 10^3 = 1000 possible states. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. [10 pts] How many possible unique actions are there for player 1 to take as their first action in a Nim game with 10 items per pile and 3 piles total? (This problem also requires a number for its answer, not merely a closed-form expression.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There would be 10*3 = 30 possible actions for the first player on their first step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. [10 pts] Do you think a Q-learner can beat the Guru player? Why or why not? Be thorough.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, if a random player can beat a Guru player on occasion, then a Q-learner can beat a guru player. More interestingly, if a Q-learning is trained with a Guru player as the opponent, it may be able to learn how to exploit the Guru policy to gain better odds against the Guru than the random player. Finally, given that the Guru's policy belongs to the set of possible policies, then there exists a set of experiences such that a Q-learner could learn the exact Guru policy and thus have the same level of success as guru vs guru. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. [40 pts] Find a way to improve the provided Nim game learning model. (Hint: How about penalizing the losses? Hint: It is indeed possible to find a better solution, which improves the way Q-learning updates its Q-table). You must code a solution and also demonstrate the improvement by reporting its performance against players (Random, Guru). Do not put the Guru playerâ€™s operating code inside the learning module, as this would defeat the purpose of reinforcement learning. However, you may train your improved Q-learner by having it playing against a Guru; using those games as experience is legitimate reinforcement learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASELINE Q-Learner Performance\n",
      "1000 games,   Random  294  Qlearner  706\n",
      "1000 games, Qlearner  681    Random  319\n",
      "1000 games,     Guru  998  Qlearner    2\n",
      "1000 games, Qlearner   14      Guru  986\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(14, 986)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## CODE FROM MODULE 9\n",
    "\n",
    "import numpy as np\n",
    "from random import randint, choice\n",
    "\n",
    "# The number of piles is 3\n",
    "# max number of items per pile\n",
    "ITEMS_MX = 10\n",
    "\n",
    "# Initialize starting position\n",
    "def init_game()->list:\n",
    "    return [randint(1,ITEMS_MX), randint(1,ITEMS_MX), randint(1,ITEMS_MX)]\n",
    "\n",
    "# Based on X-oring the item counts in piles - mathematical solution\n",
    "def nim_guru(_st:list):\n",
    "    xored = _st[0] ^ _st[1] ^ _st[2]\n",
    "    if xored == 0:\n",
    "        return nim_random(_st)\n",
    "    for pile in range(3):\n",
    "        s = _st[pile] ^ xored\n",
    "        if s <= _st[pile]:\n",
    "            return _st[pile]-s, pile\n",
    "\n",
    "# Random Nim player\n",
    "def nim_random(_st:list):\n",
    "    pile = choice([i for i in range(3) if _st[i]>0])  # find the non-empty piles\n",
    "    return randint(1, _st[pile]), pile  # random move\n",
    "\n",
    "def nim_qlearner(_st:list):\n",
    "    global qtable\n",
    "    # pick the best rewarding move, equation 1\n",
    "    a = np.argmax(qtable[_st[0], _st[1], _st[2]])  # exploitation\n",
    "    # index is based on move, pile\n",
    "    move, pile = a%ITEMS_MX+1, a//ITEMS_MX\n",
    "    # check if qtable has generated a random but game illegal move - we have not explored there yet\n",
    "    if move <= 0 or _st[pile] < move:\n",
    "        move, pile = nim_random(_st)  # exploration\n",
    "    return move, pile  # action\n",
    "\n",
    "Engines = {'Random':nim_random, 'Guru':nim_guru, 'Qlearner':nim_qlearner}\n",
    "\n",
    "def game(_a:str, _b:str):\n",
    "    state, side = init_game(), 'A'\n",
    "    while True:\n",
    "        engine = Engines[_a] if side == 'A' else Engines[_b]\n",
    "        move, pile = engine(state)\n",
    "        # print(state, move, pile)  # debug purposes\n",
    "        state[pile] -= move\n",
    "        if state == [0, 0, 0]:  # game ends\n",
    "            return side  # winning side\n",
    "        side = 'B' if side == 'A' else 'A'  # switch sides\n",
    "\n",
    "def play_games(_n:int, _a:str, _b:str):\n",
    "    from collections import defaultdict\n",
    "    wins = defaultdict(int)\n",
    "    for _ in range(_n):\n",
    "        wins[game(_a, _b)] += 1\n",
    "    # info\n",
    "    print(f\"{_n} games, {_a:>8s}{wins['A']:5d}  {_b:>8s}{wins['B']:5d}\")\n",
    "    return wins['A'], wins['B']\n",
    "\n",
    "qtable, Alpha, Gamma, Reward = None, 1.0, 0.8, 100.0\n",
    "\n",
    "# learn from _n games, randomly played to explore the possible states\n",
    "def nim_qlearn(_n:int, actor=nim_random):\n",
    "    global qtable\n",
    "    # based on max items per pile\n",
    "    qtable = np.zeros((ITEMS_MX+1, ITEMS_MX+1, ITEMS_MX+1, ITEMS_MX*3), dtype=np.float32)\n",
    "    # play _n games\n",
    "    for _ in range(_n):\n",
    "        # first state is starting position\n",
    "        st1 = init_game()\n",
    "        while True:  # while game not finished\n",
    "            # make a random move - exploration\n",
    "            move, pile = actor(st1)\n",
    "            st2 = list(st1)\n",
    "            # make the move\n",
    "            st2[pile] -= move  # --> last move I made\n",
    "            if st2 == [0, 0, 0]:  # game ends\n",
    "                qtable_update(Reward, st1, move, pile, 0)  # I won\n",
    "                break  # new game\n",
    "\n",
    "            qtable_update(0, st1, move, pile, np.max(qtable[st2[0], st2[1], st2[2]]))\n",
    "            \n",
    "            # Switch sides for play and learning\n",
    "            st1 = st2\n",
    "\n",
    "# Equation 3 - update the qtable\n",
    "def qtable_update(r:float, _st1:list, move:int, pile:int, q_future_best:float):\n",
    "    a = pile*ITEMS_MX+move-1\n",
    "    qtable[_st1[0], _st1[1], _st1[2], a] = Alpha * (r + Gamma * q_future_best)\n",
    "\n",
    "## LEARN\n",
    "nim_qlearn(1000)\n",
    "\n",
    "# Play games\n",
    "print(\"BASELINE Q-Learner Performance\")\n",
    "play_games(1000, 'Random', 'Qlearner')\n",
    "play_games(1000, 'Qlearner', 'Random')\n",
    "play_games(1000, 'Guru', 'Qlearner')\n",
    "play_games(1000, 'Qlearner', 'Guru')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reward scheme seems sufficient for performance. I tried negative rewards per-step until winning and that did not improve performance. I also tried a large negative penalty (-100) each time the agent took an action that resulted in a state with single remaining pile. This state would immediately allow the opponent to win, and thus can be considered a loss state. However, I also did not see a meaningful increase in performance from that reward scheme. \n",
    "\n",
    "The approach I have taken below focuses on the generation of experience, with the goal being to expose the Q-learner to as many possible state-action pairs as possible first using the random agent, and then to fine tune the Q-learner using the guru agent to generate experience and update the Q table with quality values for the optimal actions with the goal being to produce an agent to match the guru. \n",
    "I modified the nim_qlearn function to take in an actor function (random or guru) as an argument with which to generate experience. I also trained with each for 10k steps rather than the original 1k. This appears to have worked as shown below, with the guru agent and q-learner having nearly the same win-loss ratio against each other and themselves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASELINE Q-Learner Performance\n",
      "1000 games,   Random   10  Qlearner  990\n",
      "1000 games, Qlearner  999    Random    1\n",
      "1000 games,     Guru  936  Qlearner   64\n",
      "1000 games, Qlearner  938      Guru   62\n",
      "1000 games,     Guru  927      Guru   73\n",
      "1000 games, Qlearner  937  Qlearner   63\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(937, 63)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## CUSTOM APPROACH - RESET Q-TABLE\n",
    "qtable, Alpha, Gamma, Reward = None, 1.0, 0.8, 100.0\n",
    "\n",
    "## LEARN VIA EXPLORATION\n",
    "nim_qlearn(10000, nim_random)\n",
    "## LEARN VIA EXPLOITATION\n",
    "nim_qlearn(10000, nim_guru)\n",
    "\n",
    "# Play games\n",
    "print(\"BASELINE Q-Learner Performance\")\n",
    "play_games(1000, 'Random', 'Qlearner')\n",
    "play_games(1000, 'Qlearner', 'Random')\n",
    "play_games(1000, 'Guru', 'Qlearner')\n",
    "play_games(1000, 'Qlearner', 'Guru')\n",
    "play_games(1000, 'Guru', 'Guru')\n",
    "play_games(1000, 'Qlearner', 'Qlearner')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
